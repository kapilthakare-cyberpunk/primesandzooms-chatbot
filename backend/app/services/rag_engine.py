"""RAG (Retrieval-Augmented Generation) engine."""

from typing import List, Dict, Any, AsyncGenerator

from app.services.vector_store import VectorStore
from app.services.llm_client import LLMClient
from app.prompts.templates import SYSTEM_PROMPT, build_context_prompt
from app.config import settings


class RAGEngine:
    """Orchestrates the RAG pipeline: retrieve relevant docs, then generate response."""
    
    def __init__(self, vector_store: VectorStore):
        """
        Create a RAGEngine bound to the given vector store and initialize an LLM client.
        
        Parameters:
            vector_store (VectorStore): Vector store used to retrieve documents for retrieval-augmented generation.
        """
        self.vector_store = vector_store
        self.llm_client = LLMClient()
    
    async def query(self, user_message: str) -> Dict[str, Any]:
        """
        Run retrieval-augmented generation for a user query and return the generated answer with its sources.
        
        Parameters:
            user_message: The user's question or prompt to process.
        
        Returns:
            dict: A dictionary with keys:
                - "response": The text generated by the LLM based on retrieved context and the question.
                - "sources": A list of unique source identifiers extracted from the retrieved documents' metadata.
        """
        # Step 1: Retrieve relevant documents
        retrieved_docs = self.vector_store.similarity_search(
            query=user_message,
            k=settings.RETRIEVAL_TOP_K
        )
        
        # Step 2: Build context from retrieved documents
        context = build_context_prompt(retrieved_docs)
        sources = list(set(doc.metadata.get("source", "") for doc in retrieved_docs if doc.metadata.get("source")))
        
        # Step 3: Generate response using LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_message}"}
        ]
        
        response = await self.llm_client.chat(messages)
        
        return {
            "response": response,
            "sources": sources
        }
    
    async def query_stream(self, user_message: str) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stream a response to a user query by retrieving context and yielding tokens as they arrive.
        
        Parameters:
            user_message (str): The user's question or message to answer.
        
        Yields:
            dict: If `type` is `"token"`, contains `{"type": "token", "content": <str>}` where `content` is a generated token chunk.
                  If `type` is `"done"`, contains `{"type": "done", "sources": <List[str]>}` where `sources` is the list of unique source identifiers used to build the context.
        """
        # Step 1: Retrieve relevant documents
        retrieved_docs = self.vector_store.similarity_search(
            query=user_message,
            k=settings.RETRIEVAL_TOP_K
        )
        
        # Step 2: Build context from retrieved documents
        context = build_context_prompt(retrieved_docs)
        sources = list(set(doc.metadata.get("source", "") for doc in retrieved_docs if doc.metadata.get("source")))
        
        # Step 3: Stream response from LLM
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {user_message}"}
        ]
        
        async for token in self.llm_client.chat_stream(messages):
            yield {"type": "token", "content": token}
        
        yield {"type": "done", "sources": sources}